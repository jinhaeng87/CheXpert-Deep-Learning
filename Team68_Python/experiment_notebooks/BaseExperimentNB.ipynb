{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Pertaining to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rygu\\Anaconda3\\envs\\mldl\\lib\\site-packages\\dask\\config.py:161: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#import missingno as mso\n",
    "#import pandas_profiling as pdp\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "#from torchvision.utils import make_grid\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "#import torchsummary\n",
    "# local files\n",
    "from plotting import show_batch, plot_learning_curves\n",
    "from modeling import CheXModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True\n",
    "ON_AWS = True\n",
    "NUM_WORKERS = 4 if ON_AWS else 0 \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() and USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 404\n",
    "def seed_everything(seed=SEED, env=None):\n",
    "    random.seed(seed)\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = Path('data/')\n",
    "chxpath = PATH_DATA/'CheXpert-v1.0-small/'\n",
    "path_train = chxpath/'train'\n",
    "path_valid = chxpath/'valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LABELS = ['No_Finding', 'Enlarged_Cardiomediastinum', 'Cardiomegaly',\n",
    "       'Lung_Opacity', 'Lung_Lesion', 'Edema', 'Consolidation', 'Pneumonia',\n",
    "       'Atelectasis', 'Pneumothorax', 'Pleural_Effusion', 'Pleural_Other',\n",
    "       'Fracture', 'Support_Devices']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(chxpath/'train.csv')\n",
    "df_valid = pd.read_csv(chxpath/'valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     6,
     8,
     10,
     12,
     15,
     17
    ]
   },
   "outputs": [],
   "source": [
    "def label_smooth(df, method='uones', smooth_bounds=None):\n",
    "    df_sub = df.copy()\n",
    "    if smooth_bounds is None:\n",
    "        eps = 1e-5\n",
    "        if method == 'uones':\n",
    "            smooth_bounds = (0.55, 0.85+eps)\n",
    "        elif method=='uzeros':\n",
    "            smooth_bounds = (0, 0.30+eps)\n",
    "        else:\n",
    "            smooth_bounds = (0, 0.85+eps)\n",
    "    \n",
    "    if method in ['uones','uzeros']:\n",
    "        smooth_distrb = np.random.uniform(*smooth_bounds,df_sub[TARGET_LABELS].shape)\n",
    "        df_sub.loc[:,TARGET_LABELS] = np.where(df_sub[TARGET_LABELS]==-1, smooth_distrb, df_sub[TARGET_LABELS])\n",
    "        \n",
    "    return df_sub\n",
    "\n",
    "def proc_df(df, method='uones', smooth=True, nafill_val=0, ufill_val=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Preprocess dataframe for model consumption\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): dataframe containing img paths, metadata, and labels.\n",
    "        method (str), ('uzeros','uones','constant'): method for replacing uncertainty labels (default: 'uones')\n",
    "        smooth (bool): use Label Smoothing Regression (LSR) only applies when `method`=('uzeros','uones') (default: True) \n",
    "        nafill_val (int,float): value used to fill nan values (default: 0)\n",
    "        ufill_val (int,float): value used to fill -1 (uncertainty) labels\n",
    "    \n",
    "    kwargs:\n",
    "        smooth_bounds (tuple(float,float)): replace -1 labels uniform random values between the given bounds\n",
    "        (default: `method`='uzeros': (0,0.3001), `method`='uones': (0.55,0.8501) when `Smooth`=True, \n",
    "                \n",
    "    Returns:\n",
    "        processed_df: pandas.Dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    is_val = df['Path'].str.contains('valid').all()\n",
    "    df_sub = df.rename(lambda x: x.replace(' ','_'), axis=1).drop(columns=['Sex','Age','Frontal/Lateral','AP/PA'])\n",
    "    \n",
    "    if is_val:\n",
    "        return df_sub # val set has no nans, no -1s\n",
    "    \n",
    "    df_targets = df_sub[TARGET_LABELS]\n",
    "    \n",
    "    if isinstance(nafill_val,tuple):\n",
    "        nan_smooth_distrb = np.random.uniform(*nafill_val, df_targets.shape)\n",
    "        df_sub.loc[:,TARGET_LABELS] = np.where(df_targets.isna(), nan_smooth_distrb, df_targets)\n",
    "    else:\n",
    "        df_sub = df_sub.fillna(nafill_val)\n",
    "    \n",
    "    if smooth:\n",
    "        df_sub = label_smooth(df_sub, method, kwargs.get('smooth_bounds'))\n",
    "    elif method == 'constant':\n",
    "        df_sub = df_sub.replace(-1,ufill_val)\n",
    "    elif method in ['uzeros','uones'] and ufill_val not in [0.0,1.0]:\n",
    "        print(f'WARNING: Overwritting `ufill_val` to match method \"{method}\"')\n",
    "        ufill_val = 1.0 if method=='uones' else 0.0\n",
    "        df_sub = df_sub.replace(-1,ufill_val)\n",
    "    \n",
    "    df_sub.loc[:,TARGET_LABELS] = df_sub.loc[:,TARGET_LABELS].astype(float)\n",
    "    \n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "FRONTAL_TEMPLATE1c = cv2.imread('templates/fnt_ext_template244.jpg', 0)\n",
    "LATERAL_TEMPLATE1c = cv2.imread('templates/lat_ext_template244.jpg', 0)\n",
    "\n",
    "def template_match(img, template, tm_method=cv2.TM_CCOEFF_NORMED, init_resize=(256,256)):\n",
    "    h,w = template.shape\n",
    "    if img.ndim > 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    img1c256 = cv2.resize(img, init_resize, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    res = cv2.matchTemplate(img1c256, template, tm_method)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "    top_left = max_loc if tm_method not in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED] else min_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1]+h)\n",
    "    (x,y),(x1,y1) = top_left, bottom_right\n",
    "    \n",
    "    return cv2.cvtColor(img1c256[y:y1, x:x1], cv2.COLOR_GRAY2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TemplateCrop(A.ImageOnlyTransform):\n",
    "    \"\"\"Match image to template of either a lateral or frontal view, resizing and cropping in the process.\n",
    "    Args:\n",
    "        tm_method (int): Template matching method\n",
    "    Targets:\n",
    "        image\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tm_method=cv2.TM_CCOEFF_NORMED, init_resize=(256,256), always_apply=False, p=1.0):\n",
    "        super(TemplateCrop, self).__init__(always_apply, p)\n",
    "        self.tm_method = tm_method\n",
    "        self.init_resize = init_resize\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        template = LATERAL_TEMPLATE1c if params.get('is_lateral') else FRONTAL_TEMPLATE1c\n",
    "        return template_match(image, template, self.tm_method, self.init_resize)\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return (\"tm_method\",\"init_resize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def advprop(image,**kwargs):\n",
    "    return image*2.0-1.0\n",
    "def get_transforms(varient='train', tfms_lib='albu', imgsize=(244,244)):\n",
    "    if tfms_lib == 'albu':\n",
    "        transform = A.Compose([\n",
    "            A.RandomScale((-0.02,0.02)),\n",
    "            A.OneOf([\n",
    "                TemplateCrop(init_resize=(256, 256), p=1.0),\n",
    "                A.Compose([A.Resize(256,256, p=1.0),A.CenterCrop(*imgsize, p=1.0)])\n",
    "            ],p=1.0),\n",
    "            A.CLAHE(p=0.5),\n",
    "            A.HorizontalFlip(),\n",
    "            A.Rotate((-7,7)),#,border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.IAAAffine(shear=(-5,5)),\n",
    "            A.Cutout(8,8,8),\n",
    "            \n",
    "            #A.Lambda(advprop),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        tta_augments = A.Compose([\n",
    "            A.OneOf([\n",
    "                TemplateCrop(init_resize=(256, 256), p=1.0),\n",
    "                A.Compose([A.Resize(256,256, p=1.0), A.CenterCrop(*imgsize,p=1.0)])\n",
    "            ],p=1.0),\n",
    "            A.OneOf([\n",
    "                A.HorizontalFlip(),\n",
    "                A.Rotate((-7,7)),#border_mode=cv2.BORDER_CONSTANT),\n",
    "                A.IAAAffine(shear=(-5,5)),\n",
    "                A.NoOp()\n",
    "            ],p=1.0),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "    elif tfms_lib == 'torch':\n",
    "        transform = T.Compose([\n",
    "            #T.RandomCrop(512,8,padding_mode='reflect') ,\n",
    "            T.CenterCrop(imgsize),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(7),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "            #T.RandomErasing(inplace=True)\n",
    "        ])\n",
    "        tta_augments = T.Compose([T.CenterCrop(32),T.ToTensor()])\n",
    "        \n",
    "    return transform if varient == 'train' else tta_augments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset / DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (blank for unmentioned, 0 for negative, -1 for uncertain, and 1 for positive)\n",
    "class CheXDataset(Dataset):\n",
    "    def __init__(self, df, use_albu=True, tfms=None, smooth_bounds=None, seed=None, dpath=PATH_DATA):\n",
    "        \n",
    "        self.df = df\n",
    "        self.paths = self.df['Path'].values\n",
    "        self.labels = self.df.iloc[:,1:].values.astype(float)\n",
    "        #self.dpath = dpath\n",
    "        self.tfms = tfms\n",
    "        \n",
    "        self.use_albu = use_albu\n",
    "        self.smooth_bounds = smooth_bounds\n",
    "        self._seedcntr=seed\n",
    "        if self.tfms is not None:\n",
    "            self.tfm_list = self.tfms.transforms.transforms if self.use_albu else self.tfms.transforms \n",
    "    \n",
    "    def __len__(self,):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if self._seedcntr is not None:\n",
    "            random.seed(self._seedcntr)\n",
    "            self._seedcntr+=1\n",
    "        labels = self.labels[idx]\n",
    "        if self.smooth_bounds is not None:\n",
    "            labels = np.where(labels==-1.,np.random.uniform(*self.smooth_bounds, size=len(labels)),labels)\n",
    "        \n",
    "        imgpath = str(PATH_DATA/self.paths[idx])\n",
    "        \n",
    "        is_lateral = 'lateral' in imgpath\n",
    "        if self.use_albu:\n",
    "            img = cv2.imread(imgpath)\n",
    "            #img = template_match(img, template)\n",
    "            #img = cv2.cvtColor(cv2.imread(imgpath), cv2.COLOR_BGR2RGB)\n",
    "            #img = np.expand_dims(cv2.imread(imgpath,cv2.IMREAD_GRAYSCALE),2)\n",
    "            aug = self.tfms(image=img, is_lateral=is_lateral)\n",
    "            img = aug['image']\n",
    "        else:\n",
    "            img = Image.open(imgpath)\n",
    "            img = self.tfms(img)\n",
    "        \n",
    "        return img,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dloaders(batch_size=32, sampsz=None, tfmlib='albu', seed=None, proc_kwargs=None):\n",
    "    if seed is not None:\n",
    "        seed_everything(seed)\n",
    "\n",
    "    if proc_kwargs is None:\n",
    "        proc_kwargs = dict(method='uones',smooth=True, nafill_val=0)\n",
    "    \n",
    "    df_trn = proc_df(df_train, **proc_kwargs)\n",
    "    df_val = proc_df(df_valid)\n",
    "    if sampsz is not None:\n",
    "        df_trn = df_trn.sample(sampsz) #17->65k, 13->8k\n",
    "    \n",
    "    ualbu = (tfmlib=='albu')\n",
    "    train_tfm = get_transforms('train', tfmlib, (244,244))\n",
    "    valid_tfm = get_transforms('test', tfmlib, (244,244))\n",
    "\n",
    "    train_dataset = CheXDataset(df=df_trn, use_albu=ualbu, tfms=train_tfm, seed=seed)#smooth_bounds=(0.55,0.8501)\n",
    "    valid_dataset = CheXDataset(df=df_val, use_albu=ualbu, tfms=valid_tfm)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, pin_memory=USE_CUDA, num_workers=NUM_WORKERS)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size, pin_memory=USE_CUDA, num_workers=NUM_WORKERS, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_auc(out,target):\n",
    "    targ = target.round().detach().to('cpu')\n",
    "    out = torch.sigmoid(out).detach().to('cpu')\n",
    "    score = roc_auc_score(targ, out, average='micro',multi_class='ovo')\n",
    "\n",
    "    return score\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_auc_dbg(out,target,ninv=0):\n",
    "    try:\n",
    "        targ = target.round().detach().to('cpu')\n",
    "        out = torch.sigmoid(out).detach().to('cpu')#out.detach().to('cpu')#F.softmax(out,0).detach().to('cpu')\n",
    "        #score = roc_auc_score(targ, out, multi_class='ovo')\n",
    "        score = roc_auc_score(targ, out, average='micro',multi_class='ovo')\n",
    "    except ValueError as e:\n",
    "        score = 0.5\n",
    "        ninv+=1\n",
    "        #score = 0.5\n",
    "    return score,ninv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def save_history(history, save_name, description, save_path='save/histories/'):\n",
    "    full_path = Path(save_path)\n",
    "    full_path.mkdir(parents=True, exist_ok=True)\n",
    "    desc = f'{save_name} - {description} \\n'\n",
    "    with full_path.joinpath('description.txt').open('a+') as f:\n",
    "        f.write(desc)\n",
    "    dump_path = full_path.joinpath(save_name).with_suffix('.pkl')    \n",
    "    pickle.dump(history,dump_path.open('wb'))\n",
    "    print('File saved to:',str(dump_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "code_folding": [
     0,
     17
    ]
   },
   "outputs": [],
   "source": [
    "def save_trained(model, save_name, description=None, module='network', save_path='save/models/'):\n",
    "    full_path = Path(save_path)\n",
    "    full_path.mkdir(parents=True, exist_ok=True)\n",
    "    modelmod = getattr(model,module,model)\n",
    "    states = OrderedDict(\n",
    "        {n: c.state_dict() for n,c in modelmod.named_children() if any(p.requires_grad for p in c.parameters())}\n",
    "    )\n",
    "    if description is not None:\n",
    "        desc = f\"{save_name} - ({', '.join(states.keys())}) : {description} \\n\"\n",
    "        with full_path.joinpath('description.txt').open('a+') as f:\n",
    "            f.write(desc)\n",
    "    \n",
    "    out_path = full_path.joinpath(save_name).with_suffix('.pt')\n",
    "    torch.save(states,out_path)\n",
    "    \n",
    "    print('state dict saved to:',out_path.as_posix())\n",
    "    \n",
    "def load_trained(model, save_name, module='network', save_path='save/models/'):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    model = CheXModel('densenet121')\n",
    "    model = load_trained(model,'densenet121.pt')\n",
    "    \"\"\"\n",
    "    load_path = Path(save_path).joinpath(save_name).with_suffix('.pt')\n",
    "    saved_odict = torch.load(load_path)\n",
    "    modelmod = getattr(model,module,model)\n",
    "    \n",
    "    for k,v in saved_odict.items():\n",
    "        getattr(modelmod,k).load_state_dict(v)\n",
    "        \n",
    "    model.to(DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TrainerBase:\n",
    "    def __init__(self):\n",
    "        self.history = {}\n",
    "    \n",
    "    def freeze(self, param_names=None, invert=True, unfreeze=False):\n",
    "        init_trainables = np.array([p.requires_grad for p in self.model.parameters()])\n",
    "        trainable_params = np.array([n for n,p in self.model.named_parameters()])[init_trainables]\n",
    "        n_params = len(init_trainables)\n",
    "\n",
    "        child_names,children = zip(*[*self.model.named_children()])\n",
    "        n_child = len(children)\n",
    "        \n",
    "        if param_names is None:\n",
    "            child_train = [[x.requires_grad for x in child.parameters()] for child in self.model.children()]\n",
    "            lay_df = pd.DataFrame({'Name':child_names,'Trainable': [f'{sum(c)}/{len(c)}' for c in child_train]})#.set_index('Name')\n",
    "            print(lay_df)\n",
    "\n",
    "            print('Frozen Parameters: ({} / {})'.format((~init_trainables).sum(),n_params))\n",
    "            print('Trainable Parameters: ({} / {})'.format(init_trainables.sum(),n_params))\n",
    "            return\n",
    "        \n",
    "        params_status = {'trainable':[],'frozen':[]}\n",
    "        for name,param in self.model.named_parameters():\n",
    "            if all(map(lambda x: x not in name, param_names)):\n",
    "                if invert:\n",
    "                    if name in trainable_params:\n",
    "                        params_status['frozen'].append(name)\n",
    "                    param.requires_grad = unfreeze\n",
    "                else:\n",
    "                    param.requires_grad = !unfreeze\n",
    "            else:\n",
    "                params_status['trainable'].append(name)\n",
    "        print(f'Trainable: {len(params_status[\"trainable\"])}, Frozen: {len(params_status[\"frozen\"])}')\n",
    "        return params_status\n",
    "\n",
    "        \n",
    "    def update_history(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            self.history.setdefault(k,[]).append(v)\n",
    "            \n",
    "    def to_device(self, data, device):\n",
    "        \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "        if isinstance(data, (list,tuple)):\n",
    "            return [self.to_device(x, device) for x in data]\n",
    "        return data.to(device, non_blocking=True)\n",
    "    \n",
    "    def save_improved(self, score, best_score, save_name=None, save_path='saves/models'):\n",
    "        if score > best_score:\n",
    "            print(f'Score improved: {score:.5f} > {best_score:.5f}')\n",
    "            best_score = score\n",
    "            if save_name is not None:\n",
    "                save_trained(self.model, save_name, save_path=save_path)\n",
    "                #torch.save(self.model.state_dict(), Path(save_path)/save_name)\n",
    "        return best_score\n",
    "    \n",
    "    def train_batch_end(self, i, log_freq, **kwargs):\n",
    "        if log_freq is not None and i % log_freq==0:\n",
    "            self.update_history(**kwargs)\n",
    "    \n",
    "    def train_epoch_end(self, **kwargs):\n",
    "        self.update_history(**kwargs)#{'train_loss':loss,'train_auc':auc})\n",
    "    \n",
    "    def validation_epoch_end(self, **kwargs):\n",
    "        self.update_history(**kwargs)#{'valid_loss':loss, 'valid_auc':auc}\n",
    "    \n",
    "    def epoch_end(self, epoch, exclude_keys=None):\n",
    "        if exclude_keys is None:\n",
    "            exclude_keys = ()\n",
    "        hist_str = f'Epoch [{epoch}] '+', '.join([f'{k}: {v[-1]:.4f}' for k,v in self.history.items() if k not in exclude_keys])\n",
    "        print(hist_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer(TrainerBase):\n",
    "    def __init__(self, model, optimizer, criterion, scheduler=None, device=DEVICE):\n",
    "        super(Trainer,self).__init__()\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion.to(self.device)\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def train(self, train_loader, valid_loader, n_epochs=1, log_freq=None, save_name=None):\n",
    "        best_val_auc = 0.0\n",
    "        \n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        for epoch in pbar:\n",
    "            train_loss, train_auc = self.train_one(train_loader, pbar, log_freq)\n",
    "            self.train_epoch_end(train_loss=train_loss, train_auc=train_auc)\n",
    "            \n",
    "            valid_loss, valid_auc = self.evaluate(valid_loader, pbar)\n",
    "            self.validation_epoch_end(valid_loss=valid_loss, valid_auc=valid_auc)\n",
    "            \n",
    "            best_val_auc = self.save_improved(valid_auc, best_val_auc, save_name=save_name)\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "                \n",
    "            self.epoch_end(epoch, exclude_keys=['intraepoch_tloss','intraepoch_tauc'])\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    \n",
    "    def train_one(self, data_loader, pbar, log_freq=None):\n",
    "        self.model.train()\n",
    "        tloss,tauc=0,0\n",
    "        nbat = len(data_loader)\n",
    "        for i, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            data,target = self.to_device(batch, self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "#             if self.scheduler is not None:\n",
    "#                 self.scheduler.step()\n",
    "                \n",
    "            tloss+=loss.item()\n",
    "            tauc+=compute_auc(output,target)\n",
    "            \n",
    "            itloss,itauc = tloss/(i+1),tauc/(i+1)\n",
    "            pbar.set_postfix({'TLoss': f'{itloss:.4f}','TAUC': f'{itauc:.4f}'})\n",
    "            self.train_batch_end(i,log_freq, intraepoch_tloss=itloss, intraepoch_tauc=itauc)\n",
    "        \n",
    "        return tloss/nbat, tauc/nbat\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, data_loader, pbar):\n",
    "        self.model.eval()\n",
    "    \n",
    "        vlosses,vaucs=[],[]\n",
    "        for i, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            data,target = self.to_device(batch, self.device)\n",
    "\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            vlosses.append(loss.detach().item())\n",
    "            vauc = compute_auc(output,target)\n",
    "            vaucs.append(vauc)\n",
    "            \n",
    "            pbar.set_postfix({'VLoss': f'{np.mean(vlosses):.4f}', 'VAUC': f'{np.mean(vaucs):.4f}'})\n",
    "        \n",
    "        return np.mean(vlosses), np.mean(vaucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainer(arch='densenet121', lr=1e-3, train_loader=None, scheduler=None):\n",
    "    if train_loader is not None:\n",
    "        show_batch(train_loader,denorm=True)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model = CheXModel(arch)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if scheduler=='steplr':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1, verbose=True)\n",
    "    trainer = Trainer(model, optimizer, criterion, scheduler)\n",
    "    pstat = trainer.freeze(['_fc','fc','network.classifier','classifier'],invert=True)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
